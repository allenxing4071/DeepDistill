# DeepDistill 视频分析 Skill

> **核心原则：视频不只是"有声音的文档"。视觉信息（镜头/场景/动作/风格）是内容理解的重要维度。**

## 触发词
用户说"视频分析"、"镜头"、"场景"、"风格"、"动作"、"分镜"、"转场"时执行本 Skill。

## 必须遵守的 Rules
- **R0（变更控制）**：视频分析模块属于核心链路，修改前必须征得用户同意
- **R3（模型管理）**：模型按需下载，标注 MPS 兼容性
- **R8（经验沉淀）**：视频分析相关经验追加到本文件

---

## 一、视频分析六大维度

| 维度 | 工具 | 输出 | 硬件需求 | MPS 兼容 |
|---|---|---|---|---|
| 镜头切割 | PySceneDetect | 场景切换时间点列表 | CPU/GPU | ✅ |
| 场景识别 | YOLOv8 / SAM | 每帧物体/场景标签 | GPU 8GB+ | ✅ YOLOv8 |
| 人物/动作 | MediaPipe / OpenPose | 姿态关键点 + 动作描述 | GPU/CPU | ✅ MediaPipe |
| 拍摄手法 | Video Swin / TimeSformer | 构图/景别/镜头类型 | GPU 16GB+ | ⚠️ 需验证 |
| 风格特征 | CLIP / Video Swin | 风格向量 (embedding) | GPU 16GB+ | ✅ CLIP |
| 转场/特效 | 自定义特征提取 | 转场类型标签 | CPU/GPU | ✅ |

---

## 二、处理流程

```
输入视频
  │
  ├── Step 1: 镜头切割（PySceneDetect）
  │   └── 输出：场景列表 [{start, end, duration}]
  │
  ├── Step 2: 关键帧提取（每个场景取 1-3 帧）
  │
  ├── Step 3: 逐帧/逐场景分析（可并行）
  │   ├── 场景识别 → 物体/环境标签
  │   ├── 人物/动作 → 姿态描述
  │   ├── 拍摄手法 → 景别/构图类型
  │   └── 风格特征 → 色彩/光影/节奏向量
  │
  ├── Step 4: 转场分析（场景间过渡类型）
  │
  └── Step 5: 汇总
      └── 输出：视频视觉分析报告（JSON）
```

---

## 三、输出结构

```json
{
  "scenes": [
    {
      "index": 0,
      "start_sec": 0.0,
      "end_sec": 5.2,
      "objects": ["人物", "办公桌", "电脑"],
      "scene_type": "室内/办公",
      "actions": ["打字", "说话"],
      "shot_type": "中景",
      "composition": "三分法",
      "transition_to_next": "硬切"
    }
  ],
  "style": {
    "dominant_colors": ["#2C3E50", "#ECF0F1"],
    "lighting": "自然光/柔和",
    "pace": "中等",
    "visual_impact": 0.65
  },
  "style_embedding": [0.12, -0.34, ...]
}
```

---

## 四、渐进增强策略

视频分析模块较重，按需启用：

| 级别 | 启用模块 | 适用场景 |
|---|---|---|
| off | 不启用视频分析 | 只需转录文字 |
| basic | 镜头切割 + 关键帧 | 快速了解视频结构 |
| standard | + 场景识别 + 动作识别 | 日常使用 |
| full | + 风格分析 + 拍摄手法 + 转场 | 专业级分析 |

通过配置文件 `config/default.yaml` 的 `video_analysis.level` 控制。

---

## 五、注意事项

- 长视频（>10 分钟）建议只分析关键帧，不逐帧处理
- GPU 内存不足时自动降级（full → standard → basic）
- 模型首次使用需下载，提示用户等待
- 风格向量可用于后续素材再生成（传递给 Stable Diffusion 等）

---

## 经验沉淀

<!-- 视频分析模型/兼容性/精度相关经验追加到此处 -->
